import {
  OrderDefinition,
  PostgrestFilter,
} from "@supabase-cache-helpers/postgrest-filter";
import {
  isPostgrestHasMorePaginationCacheData,
  isPostgrestPaginationCacheData,
  isAnyPostgrestResponse,
} from "@supabase-cache-helpers/postgrest-shared";
import { default as lodashMerge } from "lodash/merge";
import { default as lodashGet } from "lodash/get";
import { binarySearch } from "./binary-search";

import { calculateNewCount } from "./calculate-new-count";
import { MutatorFn, UpsertMutatorConfig } from "./types";

const ifDateGetTime = (v: unknown) => (v instanceof Date ? v.getTime() : v);

const findPosInOrderedList = <Type extends Record<string, unknown>>(
  input: Type,
  currentData: Type[],
  orderBy: OrderDefinition[]
): number => {
  return binarySearch(currentData, input, (a, b) => {
    for (const { column, ascending, nullsFirst, foreignTable } of orderBy) {
      const aValue = ifDateGetTime(
        lodashGet(a, `${foreignTable ? `${foreignTable}.` : ""}${column}`, null)
      );

      const bValue = ifDateGetTime(
        lodashGet(b, `${foreignTable ? `${foreignTable}.` : ""}${column}`, null)
      );

      // go to next if value is equals
      if (aValue === bValue) continue;

      // nullsFirst / nullsLast
      if (aValue === null || aValue === undefined) {
        return nullsFirst ? -1 : 1;
      }

      if (bValue === null || bValue === undefined) {
        return nullsFirst ? 1 : -1;
      }

      // otherwise, if we're ascending, lowest sorts first
      if (ascending) {
        return aValue < bValue ? -1 : 1;
      }

      // if descending, highest sorts first
      return aValue < bValue ? 1 : -1;
    }

    return 0;
  });
};

export const upsert = <Type extends Record<string, unknown>>(
  input: Type,
  currentData: Type[],
  primaryKeys: (keyof Type)[],
  filter: Pick<PostgrestFilter<Type>, "apply" | "hasPaths">,
  query: { orderBy: OrderDefinition[] | undefined; limit: number | undefined },
  config?: UpsertMutatorConfig<Type>
) => {
  const mergeFn = config?.merge ? config.merge : lodashMerge;

  // find item
  const itemIdx = currentData.findIndex((oldItem) =>
    primaryKeys.every((pk) => oldItem[pk] === input[pk])
  );

  let newItem = input;
  let newItemIdx = itemIdx;

  if (itemIdx !== -1) {
    // if exists, merge and remove
    newItem = mergeFn(currentData[itemIdx], input);
    currentData.splice(itemIdx, 1);
  }

  if (Array.isArray(query.orderBy) && query.orderBy.length > 1) {
    // if ordered, find new idx
    newItemIdx = findPosInOrderedList(newItem, currentData, query.orderBy);
  }

  if (!newItemIdx) {
    // default to prepend
    newItemIdx = 0;
  }

  // check that new item is still a valid member of the list and has all required paths
  if (filter.apply(newItem) && filter.hasPaths(newItem)) {
    currentData.splice(newItemIdx, 0, newItem);
  }

  return currentData;
};

export const upsertPaginated = <Type extends Record<string, unknown>>(
  input: Type,
  currentData: Type[][],
  primaryKeys: (keyof Type)[],
  filter: Pick<PostgrestFilter<Type>, "apply" | "hasPaths">,
  query: { orderBy: OrderDefinition[] | undefined; limit: number | undefined },
  config?: UpsertMutatorConfig<Type>
) => {
  // return array in chunks
  return upsert(
    input,
    [...currentData].flat(),
    primaryKeys,
    filter,
    query,
    config
  ).reduce<Type[][]>((resultArray, item, index) => {
    // default limit is 1000
    // ref: https://github.com/supabase/supabase/discussions/3765#discussioncomment-1581021
    const chunkIndex = Math.floor(index / (query.limit ?? 1000));

    if (!resultArray[chunkIndex]) {
      resultArray[chunkIndex] = []; // start a new chunk
    }

    resultArray[chunkIndex].push(item);

    return resultArray;
  }, []);
};

export const buildUpsertMutatorFn = <Type extends Record<string, unknown>>(
  input: Type,
  primaryKeys: (keyof Type)[],
  filter: Pick<PostgrestFilter<Type>, "apply" | "hasPaths">,
  query: { orderBy: OrderDefinition[] | undefined; limit: number | undefined },
  config?: UpsertMutatorConfig<Type>
): MutatorFn<Type> => {
  const merge = config?.merge ?? lodashMerge;
  return (currentData) => {
    // Return early if undefined or null
    if (!currentData) return currentData;

    if (isPostgrestHasMorePaginationCacheData<Type>(currentData)) {
      let exists = false;
      currentData.some((page, pageIdx) => {
        // Find the old item index
        const itemIdx = page.data.findIndex((oldItem: Type) =>
          primaryKeys.every((pk) => oldItem[pk] === input[pk])
        );

        // If item is in the current page, merge it
        if (itemIdx !== -1) {
          const newItem = merge(currentData[pageIdx].data[itemIdx], input);
          // Check if the item is still a valid member of the list
          if (filter.apply(newItem))
            currentData[pageIdx].data[itemIdx] = newItem;
          // if not, remove it
          else currentData[pageIdx].data.splice(itemIdx, 1);
          exists = true;
          return true;
        }
        return false;
      });
      // Only insert if input has a value for all paths selected by the current key
      if (!exists && filter.hasPaths(input)) currentData[0].data.unshift(input);
      return currentData;
    } else if (isPostgrestPaginationCacheData<Type>(currentData)) {
      let exists = false;
      currentData.some((page, pageIdx) => {
        // Find the old item index
        const itemIdx = page.findIndex((oldItem: Type) =>
          primaryKeys.every((pk) => oldItem[pk] === input[pk])
        );

        // If item is in the current page, merge it
        if (itemIdx !== -1) {
          const newItem = merge(currentData[pageIdx][itemIdx], input);
          // Check if the item is still a valid member of the list
          if (filter.apply(newItem)) currentData[pageIdx][itemIdx] = newItem;
          // if not, remove it
          else currentData[pageIdx].splice(itemIdx, 1);
          exists = true;
          return true;
        }
        return false;
      });
      // Only insert if input has a value for all paths selected by the current key
      if (!exists && filter.hasPaths(input)) currentData[0].unshift(input);
      return currentData;
    } else if (isAnyPostgrestResponse<Type>(currentData)) {
      const { data } = currentData;

      if (!Array.isArray(data)) {
        const newData = merge(data, input);
        // Check if the new data is still valid given the key
        if (!filter.apply(newData)) return { data: null };
        return {
          data: newData,
          count: calculateNewCount<Type>(currentData),
        };
      }

      const itemIdx = data.findIndex((oldItem: Type) =>
        primaryKeys.every((pk) => oldItem[pk] === input[pk])
      );

      let mode: "add" | "subtract" | undefined;
      if (itemIdx !== -1) {
        const newItem = merge(data[itemIdx], input);
        // Check if the item is still a valid member of the list
        if (filter.apply(newItem)) {
          data[itemIdx] = newItem;
        }
        // if not, remove it
        else {
          mode = "subtract";
          data.splice(itemIdx, 1);
        }
      } else if (filter.hasPaths(input)) {
        // Only insert if input has a value for all paths selected by the current key
        mode = "add";
        data.unshift(input);
      }

      return {
        data,
        count: calculateNewCount<Type>(currentData, mode),
      };
    }

    return currentData;
  };
};
